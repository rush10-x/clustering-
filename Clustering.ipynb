{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **THEORY QUESTIONS**"
      ],
      "metadata": {
        "id": "jxOa1-hsjRgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is unsupervised learning in the context of machine learning?\n",
        "- Unsupervised learning is a type of machine learning where the model is trained on data that does not have labeled outputs. The goal is to identify patterns, structures, or groupings within the data without any prior knowledge of the categories or classes. Common applications of unsupervised learning include clustering, dimensionality reduction, and anomaly detection. The algorithms analyze the input data to find hidden structures or relationships, allowing for insights that can be used for further analysis or decision-making.\n",
        "\n",
        "Q2. How does the K-Means clustering algorithm work?\n",
        "- The K-Means clustering algorithm works through the following steps:\n",
        "\n",
        " Initialization: Choose the number of clusters\n",
        " and randomly initialize\n",
        " centroids in the feature space.\n",
        "\n",
        " Assignment Step: Assign each data point to the nearest centroid based on a distance metric (usually Euclidean distance). This forms\n",
        " clusters.\n",
        "\n",
        " Update Step: Calculate the new centroids by taking the mean of all data points assigned to each cluster.\n",
        "\n",
        " Convergence Check: Repeat the assignment and update steps until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
        "\n",
        " The algorithm aims to minimize the within-cluster variance, which is the sum of squared distances between data points and their respective centroids.\n",
        "\n",
        "Q3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "- A dendrogram is a tree-like diagram that visually represents the arrangement of clusters formed through hierarchical clustering. It illustrates how clusters are merged or split at various levels of similarity or distance. The vertical axis typically represents the distance or dissimilarity between clusters, while the horizontal axis represents the individual data points or clusters.\n",
        "\n",
        " In a dendrogram, each leaf node corresponds to a data point, and the branches indicate the merging of clusters. The height at which two clusters are joined reflects the distance between them, allowing for an intuitive understanding of the relationships and hierarchy among the clusters.\n",
        "\n",
        "Q4. What is the main difference between K-Means and Hierarchical Clustering?\n",
        "- The main differences between K-Means and hierarchical clustering are:\n",
        "\n",
        " Cluster Number Specification: K-Means requires the user to specify the number of clusters\n",
        " in advance, while hierarchical clustering does not require this; it builds a hierarchy of clusters that can be cut at different levels to obtain various numbers of clusters.\n",
        "\n",
        " Algorithm Structure: K-Means is a partitional clustering method that iteratively refines clusters based on centroids, while hierarchical clustering creates a tree structure (dendrogram) that shows how clusters are formed and related.\n",
        "\n",
        " Cluster Shape: K-Means tends to form spherical clusters and may struggle with clusters of varying shapes and densities, whereas hierarchical clustering can capture more complex relationships between data points.\n",
        "\n",
        "Q5. What are the advantages of DBSCAN over K-Means?\n",
        "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise) has several advantages over K-Means:\n",
        "\n",
        " No Need for Predefined Clusters: DBSCAN does not require the number of clusters to be specified in advance, making it more flexible for discovering clusters of varying shapes and sizes.\n",
        "\n",
        " Handling Noise and Outliers: DBSCAN can effectively identify and separate noise and outliers from the clusters, labeling them as noise points, while K-Means can be heavily influenced by outliers.\n",
        "\n",
        " A rbitrary Cluster Shapes: DBSCAN can find clusters of arbitrary shapes, as it groups points based on density rather than distance to centroids, which is a limitation of K-Means.\n",
        "\n",
        " Scalability: DBSCAN can be more efficient for large datasets, especially when the data has a lot of noise, as it does not require multiple iterations like K-Means."
      ],
      "metadata": {
        "id": "ndgW8BMwtX-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. When would you use Silhouette Score in clustering?\n",
        "\n",
        "- Optimal Number of Clusters: The Silhouette Score helps in selecting the optimal number of clusters by evaluating different values of k and identifying the one that maximizes the score.\n",
        "\n",
        " Cluster Quality Assessment: It provides a quantitative measure of how well-separated and compact the clusters are, allowing for a better understanding of clustering performance.\n",
        "\n",
        " Visual Insights: Silhouette plots can visually represent the quality of clusters, making it easier to identify potential issues with clustering.\n",
        "\n",
        " Unsupervised Evaluation: It is particularly useful in unsupervised learning scenarios where true labels are not available.\n",
        "\n",
        "Q7. What are the limitations of Hierarchical Clustering?\n",
        "\n",
        "- Computational Complexity: Hierarchical clustering can be computationally expensive, especially for large datasets, as it requires calculating distances between all pairs of points.\n",
        "\n",
        " Scalability Issues: The algorithm does not scale well with increasing data size, making it less practical for very large datasets.\n",
        "\n",
        " Sensitivity to Noise and Outliers: Hierarchical clustering can be significantly affected by noise and outliers, which can distort the dendrogram and lead to misleading clusters.\n",
        "\n",
        " Dendrogram Interpretation: The interpretation of the dendrogram can be subjective, and determining the optimal number of clusters from it can be challenging.\n",
        "\n",
        "Q8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "\n",
        "- Equal Contribution: Feature scaling ensures that all features contribute equally to the distance calculations, preventing features with larger ranges from dominating the clustering process.\n",
        "\n",
        " Improved Convergence: It can lead to faster convergence of the K-Means algorithm, as the centroids will be updated more effectively when features are on a similar scale.\n",
        "\n",
        " Better Cluster Formation: Without scaling, the algorithm may produce clusters that do not accurately reflect the underlying data structure, leading to poor clustering results.\n",
        "\n",
        "Q9. How does DBSCAN identify noise points?\n",
        "\n",
        "- Density-Based Approach: DBSCAN identifies noise points by examining the density of data points in the vicinity of each point. Points that do not have enough neighboring points within a specified distance (epsilon) are classified as noise.\n",
        "\n",
        " Core Points and Border Points: Points that are not core points (which have a minimum number of neighbors) and do not belong to any cluster are labeled as noise.\n",
        "\n",
        " Robustness to Outliers: This method allows DBSCAN to effectively handle outliers, as they are naturally identified as noise rather than being forced into clusters.\n",
        "\n",
        "Q10. Define inertia in the context of K-Means.\n",
        "\n",
        "- Sum of Squared Distances: Inertia refers to the sum of squared distances between each data point and its assigned cluster centroid. It quantifies how tightly the clusters are packed.\n",
        "\n",
        " Cluster Compactness: Lower inertia values indicate more compact clusters, while higher values suggest that the clusters are spread out and less well-defined.\n",
        "\n",
        " Optimization Goal: The K-Means algorithm aims to minimize inertia during the clustering process, leading to better-defined clusters."
      ],
      "metadata": {
        "id": "vkyhat8gzBeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is the elbow method in K-Means clustering?\n",
        "\n",
        "- The elbow method is a heuristic used to determine the optimal number of clusters in K-Means clustering. It involves running the K-Means algorithm for a range of values of $ k $ (the number of clusters) and calculating the within-cluster sum of squares (WCSS) for each $ k $. The WCSS measures the compactness of the clusters; lower values indicate tighter clusters. The results are plotted on a graph with $ k $ on the x-axis and WCSS on the y-axis. The \"elbow\" point, where the rate of decrease sharply changes, suggests the optimal number of clusters, as adding more clusters beyond this point yields diminishing returns in terms of WCSS reduction.\n",
        "\n",
        "Q12. Describe the concept of \"density\" in DBSCAN?\n",
        "\n",
        "- In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), \"density\" refers to the number of data points within a specified radius (epsilon, $ \\varepsilon $) around a given point. DBSCAN identifies clusters based on the density of points: a point is considered a core point if it has at least a minimum number of neighboring points (MinPts) within the radius $ \\varepsilon $. Points that are within the $ \\varepsilon $ radius of a core point are considered part of the same cluster. Points that are not core points and do not fall within the $ \\varepsilon $ radius of any core point are classified as noise.\n",
        "\n",
        "Q13. Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "- Yes, hierarchical clustering can be used on categorical data, but it requires a different approach than when dealing with numerical data. Since traditional distance metrics like Euclidean distance are not suitable for categorical data, alternative measures such as Jaccard distance or Hamming distance can be used. These metrics assess the similarity or dissimilarity between categorical variables, allowing hierarchical clustering algorithms to group similar categories effectively.\n",
        "\n",
        "Q14. What does a negative Silhouette Score indicate?\n",
        "\n",
        "- A negative Silhouette Score indicates that a data point is likely assigned to the wrong cluster. The Silhouette Score measures how similar a point is to its own cluster compared to other clusters. It ranges from -1 to 1, where a score close to 1 indicates that the point is well-clustered, a score of 0 indicates that the point is on or very close to the decision boundary between two neighboring clusters, and a negative score suggests that the point is closer to points in another cluster than to points in its own cluster.\n",
        "\n",
        "Q15. Explain the term \"linkage criteria\" in hierarchical clustering?\n",
        "\n",
        "- Linkage criteria in hierarchical clustering refer to the method used to determine the distance between clusters when merging them. Different linkage criteria can lead to different clustering results. Common linkage methods include:\n",
        "\n",
        " Single Linkage: The distance between the closest points of two clusters.\n",
        "\n",
        " Complete Linkage: The distance between the farthest points of two clusters.\n",
        "\n",
        " Average Linkage: The average distance between all pairs of points in two clusters.\n",
        "\n",
        " Ward's Linkage: Minimizes the total within-cluster variance by merging clusters that result in the smallest increase in total variance.\n",
        "\n",
        " The choice of linkage criteria can significantly affect the shape and size of the resulting clusters"
      ],
      "metadata": {
        "id": "hzDdMpAkz6u4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "\n",
        "- Assumption of Equal Size and Shape: K-Means assumes that clusters are spherical and of similar sizes. When clusters vary significantly in size or density, K-Means may incorrectly merge smaller clusters or split larger ones.\n",
        "\n",
        " Sensitivity to Outliers: Outliers can disproportionately affect the position of centroids, leading to poor clustering results, especially in datasets with varying densities.\n",
        "\n",
        " Distance Metric Limitations: K-Means uses Euclidean distance, which may not effectively capture the true structure of clusters that are irregularly shaped or have different densities.\n",
        "\n",
        "Q17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "\n",
        "- Epsilon (ε): This parameter defines the maximum distance between two points for them to be considered as part of the same neighborhood. A smaller ε may lead to many points being classified as noise, while a larger ε can merge distinct clusters.\n",
        "\n",
        " MinPts: This parameter specifies the minimum number of points required to form a dense region (core point). A higher MinPts value can prevent the formation of small clusters, while a lower value may lead to noise being classified as clusters.\n",
        "\n",
        " Influence on Clustering: Together, these parameters determine the density of clusters and how well DBSCAN can identify clusters of varying shapes and sizes. Proper tuning is essential for optimal performance.\n",
        "\n",
        "Q18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "\n",
        "- Strategic Centroid Selection: K-Means++ improves the initialization process by selecting initial centroids that are spread out, rather than randomly. This helps in forming well-separated clusters.\n",
        "\n",
        " Reduced Likelihood of Poor Clustering: By minimizing the potential clustering error from the start, K-Means++ often leads to better clustering quality and lower within-cluster variance.\n",
        "\n",
        " Faster Convergence: The strategic placement of centroids allows the algorithm to converge more quickly, requiring fewer iterations to reach a stable solution.\n",
        "\n",
        "Q19. What is agglomerative clustering?\n",
        "\n",
        "- Hierarchical Clustering Method: Agglomerative clustering is a bottom-up approach where each data point starts as its own cluster. The algorithm iteratively merges the closest pairs of clusters based on a distance metric.\n",
        "\n",
        " Distance Metrics: Common metrics used include single-linkage (minimum distance), complete-linkage (maximum distance), and average-linkage (mean distance).\n",
        "\n",
        " Dendrogram Representation: The results can be visualized using a dendrogram, which illustrates the merging process and helps in determining the optimal number of clusters.\n",
        "\n",
        "Q20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "- Comparison of Clusters: The Silhouette Score measures how similar a data point is to its own cluster compared to other clusters, providing a more comprehensive evaluation of cluster quality.\n",
        "\n",
        " Range of Values: The score ranges from -1 to 1, where higher values indicate better-defined clusters. In contrast, inertia only measures the compactness of clusters without considering their separation.\n",
        "\n",
        " Insight into Cluster Structure: Silhouette Score helps identify whether clusters are well-separated and appropriately defined, making it a more informative metric for evaluating clustering performance"
      ],
      "metadata": {
        "id": "XUv2Cd_P2O0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "knV-Czsm4wKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21.  Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate synthetic data with 4 centers\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Step 2: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Step 3: Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X', label='Centroids')\n",
        "plt.title('K-Means Clustering (4 Centers)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_MREzIFq42xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22.  Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Display the first 10 predicted labels\n",
        "print(\"First 10 predicted labels:\", labels[:10])\n"
      ],
      "metadata": {
        "id": "Pud2os24KZYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot DBSCAN results\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Core and border points\n",
        "plt.scatter(X[labels != -1][:, 0], X[labels != -1][:, 1], c=labels[labels != -1], cmap='viridis', s=50)\n",
        "# Outliers\n",
        "plt.scatter(X[labels == -1][:, 0], X[labels == -1][:, 1], c='red', s=50, label='Outliers')\n",
        "plt.title(\"DBSCAN Clustering with make_moons Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oW7BTBmXKbLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Print the size of each cluster\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "cluster_sizes = dict(zip(unique, counts))\n",
        "print(\"Cluster sizes:\", cluster_sizes)\n"
      ],
      "metadata": {
        "id": "j-RPm40NKcf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Core and border points\n",
        "plt.scatter(X[labels != -1][:, 0], X[labels != -1][:, 1], c=labels[labels != -1], cmap='viridis', s=50)\n",
        "# Outliers\n",
        "plt.scatter(X[labels == -1][:, 0], X[labels == -1][:, 1], c='red', s=50, label='Outliers')\n",
        "plt.title(\"DBSCAN Clustering on make_circles Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fnSyXt3OKdvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Output the cluster centroids\n",
        "print(\"Cluster centroids:\\n\", kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "lsZ5ZKWEKfhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate synthetic data with varying standard deviations\n",
        "X, _ = make_blobs(n_samples=500,\n",
        "                  centers=[[0, 0], [3, 3], [0, 4]],\n",
        "                  cluster_std=[0.2, 0.5, 1.0],\n",
        "                  random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot the result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[labels != -1][:, 0], X[labels != -1][:, 1], c=labels[labels != -1], cmap='viridis', s=50)\n",
        "plt.scatter(X[labels == -1][:, 0], X[labels == -1][:, 1], c='red', s=50, label='Outliers')\n",
        "plt.title(\"DBSCAN Clustering on make_blobs Data with Varying std\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h6B8lqQsKgtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28.  Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce dimensions to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=50)\n",
        "plt.title(\"K-Means Clustering on Digits Data (2D PCA)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SSFdBrMgKiF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Evaluate silhouette scores for k = 2 to 5\n",
        "k_values = range(2, 6)\n",
        "scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    scores.append(score)\n",
        "\n",
        "# Display as a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(k_values, scores, color='skyblue')\n",
        "plt.title(\"Silhouette Scores for Different k in K-Means\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.xticks(k_values)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1bduUiSlKjWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Perform hierarchical clustering with average linkage\n",
        "linked = linkage(X, method='average')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linked, labels=iris.target, leaf_rotation=90, leaf_font_size=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (Average Linkage)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lvgBhTlkKk8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data with overlapping clusters\n",
        "X, y = make_blobs(n_samples=500, centers=3, cluster_std=1.5, random_state=42)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.predict(X)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# Create a meshgrid for decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "                     np.linspace(y_min, y_max, 500))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundaries and clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis', edgecolor='k')\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering with Decision Boundaries\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T2ehFwhwKnh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce dimensions to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca')\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[labels != -1, 0], X_tsne[labels != -1, 1], c=labels[labels != -1], cmap='tab10', s=50)\n",
        "plt.scatter(X_tsne[labels == -1, 0], X_tsne[labels == -1, 1], c='red', s=30, label='Outliers')\n",
        "plt.title(\"DBSCAN Clustering on Digits Dataset (t-SNE Reduced)\")\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TiCxYr9DKopL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with complete linkage\n",
        "agg_clust = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "labels = agg_clust.fit_predict(X)\n",
        "\n",
        "# Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.title(\"Agglomerative Clustering with Complete Linkage\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KtMCm_rwKp0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Compare inertia values for K = 2 to 6\n",
        "inertia_values = []\n",
        "k_range = range(2, 7)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the inertia values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(k_range, inertia_values, marker='o', linestyle='-')\n",
        "plt.title(\"K-Means Inertia for K = 2 to 6\")\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EPBJeZAsKrB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q35.  Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Generate synthetic concentric circles\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with single linkage\n",
        "agg_clust = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg_clust.fit_predict(X)\n",
        "\n",
        "# Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.title(\"Agglomerative Clustering (Single Linkage) on Concentric Circles\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eqNOtJDkKs0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=1.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Count the number of clusters (excluding noise)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)\n"
      ],
      "metadata": {
        "id": "OgqmAE-mKuBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Step 2: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Step 3: Plot the clusters and the cluster centers\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')  # Clustered points\n",
        "\n",
        "# Plot cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Centroids')\n",
        "\n",
        "plt.title(\"KMeans Clustering with Cluster Centers\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TayLq1QNKvOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Step 2: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Step 4: Count and print number of noise points (label = -1)\n",
        "n_noise = np.sum(labels == -1)\n",
        "print(f\"Number of noise samples: {n_noise}\")\n"
      ],
      "metadata": {
        "id": "oC9ZK1oNKwmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q39.  Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate synthetic moon-shaped data\n",
        "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Step 2: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering on make_moons Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NOZT7s9eKxqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='tab10', s=50)\n",
        "ax.set_title('Digits Dataset: PCA (3D) + KMeans Clustering')\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Twz4aEoQKy2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q41.  Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data with 5 centers\n",
        "X, y_true = make_blobs(n_samples=500, centers=5, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Evaluate using silhouette score\n",
        "score = silhouette_score(X, labels)\n",
        "print(f\"Silhouette Score: {score:.4f}\")\n",
        "\n",
        "# Plot the clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            color='black', marker='X', s=200, label='Centers')\n",
        "plt.title('KMeans Clustering on Synthetic Blobs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L5Bd6GlNK0dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Reduce dimensionality using PCA to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the clustering result\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Set1', s=30)\n",
        "plt.title('Agglomerative Clustering on Breast Cancer Dataset (PCA-reduced)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b40AmWwWK19z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q43.  Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate noisy circular data\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot side-by-side\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# KMeans\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels_kmeans, cmap='tab10', s=30)\n",
        "plt.title('KMeans Clustering')\n",
        "\n",
        "# DBSCAN\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels_dbscan, cmap='tab10', s=30)\n",
        "plt.title('DBSCAN Clustering')\n",
        "\n",
        "plt.suptitle('Clustering Comparison on Noisy Circles')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x8e7pTlzK3Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Compute silhouette scores for each sample\n",
        "silhouette_vals = silhouette_samples(X, labels)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "y_ticks = []\n",
        "y_lower = 0\n",
        "\n",
        "for i in range(3):\n",
        "    cluster_silhouette_vals = silhouette_vals[labels == i]\n",
        "    cluster_silhouette_vals.sort()\n",
        "    y_upper = y_lower + len(cluster_silhouette_vals)\n",
        "    plt.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none')\n",
        "    y_ticks.append((y_lower + y_upper) / 2)\n",
        "    y_lower = y_upper\n",
        "\n",
        "plt.axvline(silhouette_score(X, labels), color='red', linestyle='--')\n",
        "plt.yticks(y_ticks, [f'Cluster {i}' for i in range(3)])\n",
        "plt.xlabel('Silhouette Coefficient')\n",
        "plt.title('Silhouette Plot for KMeans Clustering (Iris Dataset)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IB7F0cwWK4nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage.Visualize clusters\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering with 'average' linkage\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=40)\n",
        "plt.title(\"Agglomerative Clustering with 'average' Linkage\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6KTcGPQHK5ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q46.  Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data[:, :4]  # Use first 4 features\n",
        "df = pd.DataFrame(X, columns=wine.feature_names[:4])\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "df['Cluster'] = labels\n",
        "\n",
        "# Visualize using seaborn pairplot\n",
        "sns.pairplot(df, hue='Cluster', palette='tab10', diag_kind='hist')\n"
      ],
      "metadata": {
        "id": "SeXqCaYgK7n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Generate noisy blobs\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.9, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Count clusters and noise\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = np.sum(labels == -1)\n",
        "\n",
        "print(f\"Number of clusters: {n_clusters}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n"
      ],
      "metadata": {
        "id": "35ounsb7K83-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce dimensions using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=30)\n",
        "plt.title('Agglomerative Clustering on Digits Dataset (t-SNE Reduced)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-zJvRWLimQ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qsB13ZQgm4P2"
      }
    }
  ]
}